{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLzX_EPqAnEW"
   },
   "source": [
    "### Fine-tuning 6-Billion GPT-J in colab with LoRA and 8-bit compression\n",
    "\n",
    "This notebook is a proof of concept for fine-tuning [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) with limited memory. A detailed explanation of how it works can be found in [this model card](https://huggingface.co/hivemind/gpt-j-6B-8bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Op0GXmC8CCyR",
    "outputId": "b60baee6-912d-4f2b-c161-e376d5c51f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.14.1 in /usr/local/lib/python3.7/dist-packages (4.14.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (3.4.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (4.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (2019.12.20)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (0.0.47)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.1) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.14.1) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.14.1) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.14.1) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.14.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.14.1) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.14.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.14.1) (2021.10.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.14.1) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.14.1) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.14.1) (1.15.0)\n",
      "Requirement already satisfied: bitsandbytes-cuda111==0.26.0 in /usr/local/lib/python3.7/dist-packages (0.26.0)\n",
      "Requirement already satisfied: datasets==1.16.1 in /usr/local/lib/python3.7/dist-packages (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (0.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (4.10.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (4.62.3)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (6.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (1.3.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (0.70.12.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (2022.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (1.19.5)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (3.8.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (2.0.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.16.1) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.16.1) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.16.1) (3.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.16.1) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.16.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.16.1) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.16.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.16.1) (2.10)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (2.0.11)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (1.3.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (4.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.16.1) (3.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.16.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.16.1) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.16.1) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.14.1\n",
    "!pip install bitsandbytes-cuda111==0.26.0\n",
    "!pip install datasets==1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0dy1ZFwClcq"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import custom_fwd, custom_bwd\n",
    "\n",
    "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GODiktIBFt4w"
   },
   "source": [
    "### Converting the model to 8 bits.\n",
    "\n",
    "We convert EleutherAI's GPT-J-6B model to 8 bits using facebook's [bitsandbytes](https://github.com/facebookresearch/bitsandbytes) library. This reduces the model's size from 20Gb down to just 6Gb.\n",
    "\n",
    "Note that we don't convert linear layer biases to 8 bit as they take up less that 1% of the model's weight anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8Y75B6WDIN-"
   },
   "outputs": [],
   "source": [
    "class FrozenBNBLinear(nn.Module):\n",
    "    def __init__(self, weight, absmax, code, bias=None):\n",
    "        assert isinstance(bias, nn.Parameter) or bias is None\n",
    "        super().__init__()\n",
    "        self.out_features, self.in_features = weight.shape\n",
    "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
    "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
    "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
    "        self.adapter = None\n",
    "        self.bias = bias\n",
    " \n",
    "    def forward(self, input):\n",
    "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
    "        if self.adapter:\n",
    "            output += self.adapter(input)\n",
    "        return output\n",
    " \n",
    "    @classmethod\n",
    "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
    "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
    "        return cls(weights_int8, *state, linear.bias)\n",
    " \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
    " \n",
    " \n",
    "class DequantizeAndLinear(torch.autograd.Function): \n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
    "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
    "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
    "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
    "        ctx._has_bias = bias is not None\n",
    "        return F.linear(input, weights_deq, bias)\n",
    " \n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
    "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
    "        # grad_output: [*batch, out_features]\n",
    "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
    "        grad_input = grad_output @ weights_deq\n",
    "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
    "        return grad_input, None, None, None, grad_bias\n",
    " \n",
    " \n",
    "class FrozenBNBEmbedding(nn.Module):\n",
    "    def __init__(self, weight, absmax, code):\n",
    "        super().__init__()\n",
    "        self.num_embeddings, self.embedding_dim = weight.shape\n",
    "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
    "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
    "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
    "        self.adapter = None\n",
    " \n",
    "    def forward(self, input, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            # note: both quantuized weights and input indices are *not* differentiable\n",
    "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
    "            output = F.embedding(input, weight_deq, **kwargs)\n",
    "        if self.adapter:\n",
    "            output += self.adapter(input)\n",
    "        return output \n",
    " \n",
    "    @classmethod\n",
    "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
    "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
    "        return cls(weights_int8, *state)\n",
    " \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
    " \n",
    " \n",
    "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
    "    assert chunk_size % 4096 == 0\n",
    "    code = None\n",
    "    chunks = []\n",
    "    absmaxes = []\n",
    "    flat_tensor = matrix.view(-1)\n",
    "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
    "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
    "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
    "        chunks.append(quantized_chunk)\n",
    "        absmaxes.append(absmax_chunk)\n",
    " \n",
    "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
    "    absmax = torch.cat(absmaxes)\n",
    "    return matrix_i8, (absmax, code)\n",
    " \n",
    " \n",
    "def convert_to_int8(model):\n",
    "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
    "    for module in list(model.modules()):\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                print(name, child)\n",
    "                setattr( \n",
    "                    module,\n",
    "                    name,\n",
    "                    FrozenBNBLinear(\n",
    "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
    "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
    "                        code=torch.zeros(256),\n",
    "                        bias=child.bias,\n",
    "                    ),\n",
    "                )\n",
    "            elif isinstance(child, nn.Embedding):\n",
    "                setattr(\n",
    "                    module,\n",
    "                    name,\n",
    "                    FrozenBNBEmbedding(\n",
    "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
    "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
    "                        code=torch.zeros(256),\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOSZ-S1cDRq1"
   },
   "outputs": [],
   "source": [
    "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        convert_to_int8(self.attn)\n",
    "        convert_to_int8(self.mlp)\n",
    "\n",
    "\n",
    "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        convert_to_int8(self)\n",
    "        \n",
    "\n",
    "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        convert_to_int8(self)\n",
    "\n",
    "\n",
    "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pthEhmDBSyEm"
   },
   "outputs": [],
   "source": [
    "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "15098994ad0b4e7e8d42b8e222372876",
      "66f08b4575194e19887a1b74612933a7",
      "d0632a3bcfa742fbae2f722ddbfd0590",
      "b6684cbbff294afeae9843e4e5f93eda",
      "5a41621aa4ea44d2aea9a3f082f19ba1",
      "14052b401b9d40b5ab4ceba04d296856",
      "743b551103134757b2cf319eda21579e",
      "0bd204635e8d4474872d9d901b53de5f",
      "611d49154e704af6800d1c9c13f90955",
      "a218f336760b43608f6c9096ddb6dbb8",
      "1f6eabef3df64150a792a2eb72c97681"
     ]
    },
    "id": "DuW4H6HTS82r",
    "outputId": "9d8b1aad-a676-448b-d8d7-b05b6a0ca109"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15098994ad0b4e7e8d42b8e222372876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "lm_head Linear(in_features=4096, out_features=50400, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): FrozenBNBEmbedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): FrozenBNBLinear(4096, 50400)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRF2QqvzENCr"
   },
   "source": [
    "### Text generation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "RJd45kFaZ1Ba",
    "outputId": "fe86cd05-1dec-4f09-c149-6597af18a9e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A cat sat on a mat in the middle of the pavement and glared up at a car window, its tail slowly waving.\\n\\n\"It wants to go inside the car,\" Mrs Gulliver said, stroking the cat. \"And then that car can stop a man and put its paw on the chest of the man.\"\\n\\nThe man in the back of the car said, \"So long, dusky,\" and started the engine.\\n\\n\"Dusky,\" said Mr Gulliver. \"A very nice word.\"\\n\\n\"I suppose one could argue that if a man could be beaten by the power of'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = tokenizer(\"A cat sat on a mat\", return_tensors='pt')\n",
    "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
    "out = gpt.generate(**prompt, min_length=128, max_length=128, do_sample=True)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfdLQHOuEU7h"
   },
   "source": [
    "### LoRA fine-tuning example\n",
    "Here we demonstrate how to fine-tune the proposed model using low-rank adapters [(Hu et al, 2021)](https://arxiv.org/abs/2106.09685) and [8-bit Adam](https://arxiv.org/abs/2110.02861). We also use [dataset streaming API](https://huggingface.co/docs/datasets/dataset_streaming.html) to avoid downloading the large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5ctu4Q5aq-g"
   },
   "outputs": [],
   "source": [
    "def add_adapters(model, adapter_dim=16):\n",
    "    assert adapter_dim > 0\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, FrozenBNBLinear):\n",
    "            module.adapter = nn.Sequential(\n",
    "                nn.Linear(module.in_features, adapter_dim, bias=False),\n",
    "                nn.Linear(adapter_dim, module.out_features, bias=False),\n",
    "            )\n",
    "            nn.init.zeros_(module.adapter[1].weight)\n",
    "        elif isinstance(module, FrozenBNBEmbedding):\n",
    "            module.adapter = nn.Sequential(\n",
    "                nn.Embedding(module.num_embeddings, adapter_dim),\n",
    "                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n",
    "            )\n",
    "            nn.init.zeros_(module.adapter[1].weight)\n",
    "\n",
    "add_adapters(gpt)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIlHG9Wk0WaJ"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "\n",
    "gpt.gradient_checkpointing_enable()\n",
    "\n",
    "codeparrot = load_dataset(\"transformersbook/codeparrot-train\", streaming=True)\n",
    "optimizer = Adam8bit(gpt.parameters(), lr=1e-5)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    for row in tqdm(codeparrot[\"train\"]):\n",
    "        if len(row[\"content\"]) <= 1:\n",
    "            continue\n",
    "\n",
    "        batch = tokenizer(row[\"content\"], truncation=True, max_length=128, return_tensors='pt')\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "        out = gpt.forward(**batch,)\n",
    "\n",
    "        loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch['input_ids'][:, 1:].flatten(),\n",
    "                               reduction='mean')\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx1T8Qvga-fN"
   },
   "source": [
    "This training loop is just a proof of concept - to show that even in the heaviest case, it still fits on a gpu.\n",
    "Depending on your finetuning task, you'll need to remove some parts.\n",
    "Below we explain how to modify the code to achieve the setup from the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "\n",
    "If you wanna fine-tune a-la LoRA , please use the parameters from Table 11,12 and 15 as a starter:\n",
    "\n",
    "(1) Train only the adapter matrices from attention layers\n",
    "\n",
    "In the above example, we train all kinds of adapters, and also layernorm scales and biases. This is only useful for fine-tuning over reasonably large datasets over long time.\n",
    "For quick setups you should tag everything except **the attention adapters** as `requires_grad=False` -- or just don't feed them into Adam:\n",
    "\n",
    "```\n",
    "\n",
    "params_for_optimizer = [\n",
    "    param for name, param in model.named_parameters()\n",
    "    if \"attn\" in name and \"adapter\" in name\n",
    "]\n",
    "print(\"Trainiable params:\", len(params_for_optimizer))\n",
    "\n",
    "# and after you verified it:\n",
    "for name, param in model.named_parameters():\n",
    "    if param not in params_for_optimizer:\n",
    "        print(f\"Setting {name} requires_grad=False\")\n",
    "        param.requires_grad = False\n",
    "```\n",
    "\n",
    "An even better way is to only create adapters that you need by modifying the `add_adapters` function above:\n",
    "```\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (FrozenBNBLinear, FrozenBNBEmbedding)):\n",
    "        if \"attn\" in name:\n",
    "            print(\"Adding adapter to\", name)\n",
    "\n",
    "            todo_initialize_adapters_like_in_notebook()\n",
    "        else:\n",
    "            print(\"Not adding adapter to\", name)\n",
    "```\n",
    "As a side-effect, that would actually somewhat reduce the memory usage and may let you fit a longer sequence (e.g. 256)\n",
    "\n",
    "\n",
    "(2) initialize the second adapter matrix with zeros\n",
    "```\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, \"adapter\"):\n",
    "        print(\"Initializing\", name)\n",
    "        nn.init.zeros_(module.adapter[1].weight)\n",
    "        # optional: scale adapter[0].weight by (LoRA_alpha / r)\n",
    "```\n",
    "\n",
    "(3) use warmup and weight decay in Adam:\n",
    "```\n",
    "optimizer = Adam8Bit(..., weight_decay=0.01)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps_from_paper(), expected_total_number_of_steps\n",
    ")\n",
    "\n",
    "actually_use_scheduler_in_training_loop()\n",
    "```\n",
    "\n",
    "Finally, we recommend modifying training loop to track the training metrics, saving the best checkpoint, etc."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0bd204635e8d4474872d9d901b53de5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "14052b401b9d40b5ab4ceba04d296856": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "15098994ad0b4e7e8d42b8e222372876": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0632a3bcfa742fbae2f722ddbfd0590",
       "IPY_MODEL_b6684cbbff294afeae9843e4e5f93eda",
       "IPY_MODEL_5a41621aa4ea44d2aea9a3f082f19ba1"
      ],
      "layout": "IPY_MODEL_66f08b4575194e19887a1b74612933a7"
     }
    },
    "1f6eabef3df64150a792a2eb72c97681": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a41621aa4ea44d2aea9a3f082f19ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f6eabef3df64150a792a2eb72c97681",
      "placeholder": "",
      "style": "IPY_MODEL_a218f336760b43608f6c9096ddb6dbb8",
      "value": " 5.75G/5.75G [04:09&lt;00:00, 32.5MB/s]"
     }
    },
    "611d49154e704af6800d1c9c13f90955": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66f08b4575194e19887a1b74612933a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "743b551103134757b2cf319eda21579e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a218f336760b43608f6c9096ddb6dbb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6684cbbff294afeae9843e4e5f93eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_611d49154e704af6800d1c9c13f90955",
      "max": 6177012613,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0bd204635e8d4474872d9d901b53de5f",
      "value": 6177012613
     }
    },
    "d0632a3bcfa742fbae2f722ddbfd0590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_743b551103134757b2cf319eda21579e",
      "placeholder": "",
      "style": "IPY_MODEL_14052b401b9d40b5ab4ceba04d296856",
      "value": "Downloading: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
